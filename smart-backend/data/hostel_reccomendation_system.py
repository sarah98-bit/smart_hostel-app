# -*- coding: utf-8 -*-
"""Hostel_reccomendation system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FNaKXHFFob5a23O2KAw5nt-PTlw9oT0i
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import MinMaxScaler
from scipy.sparse import hstack

"""##load dataset

"""

df = pd.read_csv("dkut_hostels.csv")
print("Dataset shape:", df.shape)
print(df.head())

"""##Content-based filtering

"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import MinMaxScaler, normalize
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse import hstack
import numpy as np

# 1) Combine stronger text features
df['cbf_text'] = (
    df['name'].fillna('') + " " +
    df['facilities'].fillna('') + " " +
    df['room_types'].fillna('') + " " +
    df.get('description', '')
)

# 2) TF-IDF with unigrams + bigrams
tfidf = TfidfVectorizer(stop_words='english', ngram_range=(1,2))
tfidf_matrix = tfidf.fit_transform(df['cbf_text'])

# 3) Boost numerical influence
df['rating_scaled'] = df['rating'] * 2.0
df['price_scaled'] = df['price_kes_per_month'] * 1.5
df['distance_scaled'] = df['distance_km'] * 1.2

# 4) Scale numeric features
num_features = df[['distance_scaled', 'price_scaled', 'rating_scaled']]
num_scaled = MinMaxScaler().fit_transform(num_features)

# 5) Weighted combined matrix
X = hstack([
    tfidf_matrix * 0.6,
    num_scaled * 0.4
])


# Normalize for fair cosine similarity
X = normalize(X)

# 6) Recommendation function
def recommend_content_based(hostel_index, top_n=5):
    sims = cosine_similarity(X[hostel_index], X).flatten()

    # small rating reward
    sims = sims + (df['rating'] / 5) * 0.10

    sims[hostel_index] = -1
    top = sims.argsort()[::-1][:top_n]

    return df.iloc[top][[
        "hostel_id","name","price_kes_per_month","facilities","rating"
    ]]

"""## Evaluation (Precision@K, Recall@K)"""

def precision_recall_at_k(X, y, k=5, rating_threshold=4.0):
    precisions, recalls = [], []
    for i in range(len(y)):
        sims = cosine_similarity(X[i], X).flatten()
        similar_indices = sims.argsort()[::-1][1:k+1]
        relevant = (y.iloc[similar_indices] >= rating_threshold).sum()
        retrieved = k
        possible_relevant = (y >= rating_threshold).sum()

        precision = relevant / retrieved if retrieved > 0 else 0
        recall = relevant / possible_relevant if possible_relevant > 0 else 0
        precisions.append(precision)
        recalls.append(recall)

    return np.mean(precisions), np.mean(recalls)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, df['rating'], test_size=0.2, random_state=42)

precision, recall = precision_recall_at_k(X_test, y_test, k=5)
print(f"Precision@5: {precision:.2f}")
print(f"Recall@5: {recall:.2f}")

"""##Collaborative filtering

"""

import random
import pandas as pd


# Assume df contains the hostels data from dkut_hostels.csv
df = pd.read_csv("dkut_hostels.csv")

print(df.head())
print(f"Hostel dataset shape: {df.shape}")
hostel_ids = df["hostel_id"].tolist()
users = [f"U{i}" for i in range(1, 501)]  # 500 synthetic users

ratings_data = []
for user in users:
    # Each user rates between 5 and 15 hostels
    rated_hostels = random.sample(hostel_ids, random.randint(5, 15))
    for hid in rated_hostels:
        ratings_data.append({
            "user_id": user,
            "hostel_id": hid,
            "rating": random.randint(1, 5)  # rating between 1‚Äì5
        })

user_ratings = pd.DataFrame(ratings_data)
print(user_ratings.head())
print(f"Shape of ratings dataset: {user_ratings.shape}")

from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

user_item = user_ratings.pivot(index='user_id', columns='hostel_id', values='rating').fillna(0)

# Apply TruncatedSVD
n_components = min(50, min(user_item.shape)-1)
svd = TruncatedSVD(n_components=n_components, random_state=42)
latent = svd.fit_transform(user_item)

# Reconstruct predicted ratings
reconstructed = np.dot(latent, svd.components_)
preds_df = pd.DataFrame(reconstructed, index=user_item.index, columns=user_item.columns)

# --- Evaluation ---
true_vals, pred_vals = [], []
for u in user_item.index:
    for h in user_item.columns:
        if user_item.loc[u, h] > 0:  # rated
            true_vals.append(user_item.loc[u, h])
            pred_vals.append(preds_df.loc[u, h])

mse = mean_squared_error(true_vals, pred_vals)
rmse = np.sqrt(mse) # Calculate RMSE manually
mae = mean_absolute_error(true_vals, pred_vals)


print(f"Collaborative Filtering (SVD via sklearn) -> RMSE: {rmse:.4f}, MAE: {mae:.4f}")

# --- Recommendation function ---
def recommend_cf_svd(user_id, top_n=5):
    preds = preds_df.loc[user_id].copy()
    rated_mask = user_item.loc[user_id] > 0
    preds[rated_mask] = -np.inf
    top_hostels = preds.nlargest(top_n).index.tolist()
    return df[df['hostel_id'].isin(top_hostels)][["hostel_id","name","price_kes_per_month","facilities","rating"]]

# Example usage
print("Collaborative Filtering Recommendations for U1:")
print(recommend_cf_svd("U1", top_n=5))

"""##Final Model Testing and Evaluation

"""

import time

"""--- CONTENT-BASED TESTS --"""

import re
print("üìä Testing Content-Based Recommendation Model")
# Test 1: Speed
start = time.time()
_ = recommend_content_based(0, top_n=5)
end = time.time()
cbf_speed = end - start
print(f"‚è± Content-Based Recommendation Speed: {cbf_speed:.4f} seconds")
assert cbf_speed < 3, "‚ùå Content-based model is too slow (should be under 3 seconds)"
# Test 2: Output Format
cbf_result = recommend_content_based(0, top_n=5)
assert isinstance(cbf_result, pd.DataFrame), "‚ùå Content-based output is not a DataFrame"
assert not cbf_result.empty, "‚ùå No recommendations returned"
print(f"‚úÖ Returned {len(cbf_result)} recommendations with expected format")

# Test 3: Cosine similarity score check
X_csr = X.tocsr()
target_vec = X_csr[0]
sims = cosine_similarity(target_vec, X_csr).flatten()

similarities = sims[cbf_result.index]
avg_similarity = similarities.mean()

print(f"üîç Average Cosine Similarity: {avg_similarity:.3f}")
assert avg_similarity > 0.2, "‚ùå Recommendations are not similar enough"


def clean(text):
    return re.sub(r'[^a-zA-Z0-9 ]+', '', text).lower().split()

target_facility = df.loc[0, 'facilities'] # Define target_facility
target_words = set(clean(target_facility))

facility_match_ratio = cbf_result['facilities'].apply(
    lambda f: len(target_words.intersection(set(clean(f)))) / max(len(target_words), 1)
).mean()

print(f"üéØ Facility text similarity: {facility_match_ratio*100:.2f}%")

"""
 --- COLLABORATIVE FILTERING TESTS ---"""

print("üìä Testing Collaborative Filtering (SVD) Model")
test_user = user_ratings['user_id'].iloc[0]

# Test 1: Speed
start = time.time()
_ = recommend_cf_svd(test_user, top_n=5)
end = time.time()
cf_speed = end - start
print(f"‚è± Collaborative Filtering Recommendation Speed: {cf_speed:.4f} seconds")
assert cf_speed < 3, "‚ùå CF model is too slow (should be under 3 seconds)"

# Test 2: Output Format
cf_result = recommend_cf_svd(test_user, top_n=5)
assert isinstance(cf_result, pd.DataFrame), "‚ùå CF output is not a DataFrame"
assert not cf_result.empty, "‚ùå CF returned no recommendations"
print(f"‚úÖ Returned {len(cf_result)} recommendations with expected format")

# Test 3: Score Sanity Check
pred_scores = preds_df.loc[test_user, cf_result['hostel_id']].values
assert all(pred_scores <= 5) and all(pred_scores >= 0), "‚ùå Predicted scores out of expected range"
print("‚úÖ Collaborative Filtering Recommendation Passed ‚úÖ\n")

"""## Final Evaluation of Content-Based and Collaborative Filtering Models"""

# --- EVALUATION SUMMARY ---
print("üìà Summary:")
print(f"  - Content-Based Speed: {cbf_speed:.4f} s")
print(f"  - Collaborative Filtering Speed: {cf_speed:.4f} s")
print(f"  - Precision@5 (Content-Based): {precision:.2f}")
print(f"  - Recall@5 (Content-Based): {recall:.2f}")
print(f"  - RMSE (Collaborative Filtering): {rmse:.4f}")
print(f"  - MAE (Collaborative Filtering): {mae:.4f}")

# --- CONTENT-BASED ACCURACY ---
p, r = precision, recall
cbf_accuracy = ((p + r) / 2) * 100

# --- COLLABORATIVE FILTERING ACCURACY ---
max_error = 4  # ratings 1‚Äì5

rmse_accuracy = (1 - (rmse / max_error)) * 100
mae_accuracy  = (1 - (mae / max_error)) * 100
cf_accuracy = (rmse_accuracy + mae_accuracy) / 2

# --- OVERALL MODEL ACCURACY ---
overall_accuracy = (cbf_accuracy * 0.5) + (cf_accuracy * 0.5)

print("Content-Based Accuracy: {:.2f}%".format(cbf_accuracy))
print("Collaborative Filtering Accuracy: {:.2f}%".format(cf_accuracy))
print("Overall Recommendation Accuracy: {:.2f}%".format(overall_accuracy))